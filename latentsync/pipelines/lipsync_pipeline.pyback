# Adapted from https://github.com/guoyww/AnimateDiff/blob/main/animatediff/pipelines/pipeline_animation.py
# lipsync_pipeline.py
import inspect
import math
import os
import shutil
from typing import Callable, List, Optional, Union
import subprocess

import numpy as np
import torch
import torchvision
from torchvision import transforms

from packaging import version

from diffusers.configuration_utils import FrozenDict
from diffusers.models import AutoencoderKL
from diffusers.pipelines import DiffusionPipeline
from diffusers.schedulers import (
    DDIMScheduler,
    DPMSolverMultistepScheduler,
    EulerAncestralDiscreteScheduler,
    EulerDiscreteScheduler,
    LMSDiscreteScheduler,
    PNDMScheduler,
)
from diffusers.utils import deprecate, logging

from einops import rearrange
import cv2

from ..models.unet import UNet3DConditionModel
from ..utils.util import read_video, read_audio, write_video, check_ffmpeg_installed
from ..utils.image_processor import ImageProcessor, load_fixed_mask
from ..whisper.audio2feature import Audio2Feature
import tqdm # Zorg ervoor dat tqdm geïmporteerd is (was al aanwezig)
import soundfile as sf

logger = logging.get_logger(__name__)  # pylint: disable=invalid-name


class LipsyncPipeline(DiffusionPipeline):
    _optional_components = []

    def __init__(
        self,
        vae: AutoencoderKL,
        audio_encoder: Audio2Feature,
        denoising_unet: UNet3DConditionModel,
        scheduler: Union[
            DDIMScheduler,
            PNDMScheduler,
            LMSDiscreteScheduler,
            EulerDiscreteScheduler,
            EulerAncestralDiscreteScheduler,
            DPMSolverMultistepScheduler,
        ],
    ):
        super().__init__()

        if hasattr(scheduler.config, "steps_offset") and scheduler.config.steps_offset != 1:
            deprecation_message = (
                f"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`"
                f" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure "
                "to update the config accordingly as leaving `steps_offset` might led to incorrect results"
                " in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,"
                " it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`"
                " file"
            )
            deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)
            new_config = dict(scheduler.config)
            new_config["steps_offset"] = 1
            scheduler._internal_dict = FrozenDict(new_config)

        if hasattr(scheduler.config, "clip_sample") and scheduler.config.clip_sample is True:
            deprecation_message = (
                f"The configuration file of this scheduler: {scheduler} has not set the configuration `clip_sample`."
                " `clip_sample` should be set to False in the configuration file. Please make sure to update the"
                " config accordingly as not setting `clip_sample` in the config might lead to incorrect results in"
                " future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very"
                " nice if you could open a Pull request for the `scheduler/scheduler_config.json` file"
            )
            deprecate("clip_sample not set", "1.0.0", deprecation_message, standard_warn=False)
            new_config = dict(scheduler.config)
            new_config["clip_sample"] = False
            scheduler._internal_dict = FrozenDict(new_config)

        is_unet_version_less_0_9_0 = hasattr(denoising_unet.config, "_diffusers_version") and version.parse(
            version.parse(denoising_unet.config._diffusers_version).base_version
        ) < version.parse("0.9.0.dev0")
        is_unet_sample_size_less_64 = (
            hasattr(denoising_unet.config, "sample_size") and denoising_unet.config.sample_size < 64
        )
        if is_unet_version_less_0_9_0 and is_unet_sample_size_less_64:
            deprecation_message = (
                "The configuration file of the unet has set the default `sample_size` to smaller than"
                " 64 which seems highly unlikely. If your checkpoint is a fine-tuned version of any of the"
                " following: \n- CompVis/stable-diffusion-v1-4 \n- CompVis/stable-diffusion-v1-3 \n-"
                " CompVis/stable-diffusion-v1-2 \n- CompVis/stable-diffusion-v1-1 \n- runwayml/stable-diffusion-v1-5"
                " \n- runwayml/stable-diffusion-inpainting \n you should change 'sample_size' to 64 in the"
                " configuration file. Please make sure to update the config accordingly as leaving `sample_size=32`"
                " in the config might lead to incorrect results in future versions. If you have downloaded this"
                " checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for"
                " the `unet/config.json` file"
            )
            deprecate("sample_size<64", "1.0.0", deprecation_message, standard_warn=False)
            new_config = dict(denoising_unet.config)
            new_config["sample_size"] = 64
            denoising_unet._internal_dict = FrozenDict(new_config)

        self.register_modules(
            vae=vae,
            audio_encoder=audio_encoder,
            denoising_unet=denoising_unet,
            scheduler=scheduler,
        )

        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)

        self.set_progress_bar_config(desc="Steps")

    def enable_vae_slicing(self):
        self.vae.enable_slicing()

    def disable_vae_slicing(self):
        self.vae.disable_slicing()

    @property
    def _execution_device(self):
        if self.device != torch.device("meta") or not hasattr(self.denoising_unet, "_hf_hook"):
            return self.device
        for module in self.denoising_unet.modules():
            if (
                hasattr(module, "_hf_hook")
                and hasattr(module._hf_hook, "execution_device")
                and module._hf_hook.execution_device is not None
            ):
                return torch.device(module._hf_hook.execution_device)
        return self.device

    def decode_latents(self, latents):
        latents = latents / self.vae.config.scaling_factor + self.vae.config.shift_factor
        latents = rearrange(latents, "b c f h w -> (b f) c h w")
        decoded_latents = self.vae.decode(latents).sample
        return decoded_latents

    def prepare_extra_step_kwargs(self, generator, eta):
        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature
        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.
        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502
        # and should be between [0, 1]

        accepts_eta = "eta" in set(inspect.signature(self.scheduler.step).parameters.keys())
        extra_step_kwargs = {}
        if accepts_eta:
            extra_step_kwargs["eta"] = eta

        # check if the scheduler accepts generator
        accepts_generator = "generator" in set(inspect.signature(self.scheduler.step).parameters.keys())
        if accepts_generator:
            extra_step_kwargs["generator"] = generator
        return extra_step_kwargs

    def check_inputs(self, height, width, callback_steps):
        assert height == width, "Height and width must be equal"

        if height % 8 != 0 or width % 8 != 0:
            raise ValueError(f"`height` and `width` have to be divisible by 8 but are {height} and {width}.")

        if (callback_steps is None) or (
            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)
        ):
            raise ValueError(
                f"`callback_steps` has to be a positive integer but is {callback_steps} of type"
                f" {type(callback_steps)}."
            )

    def prepare_latents(self, batch_size, num_frames, num_channels_latents, height, width, dtype, device, generator):
        shape = (
            batch_size,
            num_channels_latents,
            1,
            height // self.vae_scale_factor,
            width // self.vae_scale_factor,
        )
        rand_device = "cpu" if device.type == "mps" else device
        latents = torch.randn(shape, generator=generator, device=rand_device, dtype=dtype).to(device)
        latents = latents.repeat(1, 1, num_frames, 1, 1)

        # scale the initial noise by the standard deviation required by the scheduler
        latents = latents * self.scheduler.init_noise_sigma
        return latents

    def prepare_mask_latents(
        self, mask, masked_image, height, width, dtype, device, generator, do_classifier_free_guidance
    ):
        # resize the mask to latents shape as we concatenate the mask to the latents
        # we do that before converting to dtype to avoid breaking in case we're using cpu_offload
        # and half precision
        mask = torch.nn.functional.interpolate(
            mask, size=(height // self.vae_scale_factor, width // self.vae_scale_factor)
        )
        masked_image = masked_image.to(device=device, dtype=dtype)

        # encode the mask image into latents space so we can concatenate it to the latents
        masked_image_latents = self.vae.encode(masked_image).latent_dist.sample(generator=generator)
        masked_image_latents = (masked_image_latents - self.vae.config.shift_factor) * self.vae.config.scaling_factor

        # aligning device to prevent device errors when concating it with the latent model input
        masked_image_latents = masked_image_latents.to(device=device, dtype=dtype)
        mask = mask.to(device=device, dtype=dtype)

        # assume batch size = 1
        mask = rearrange(mask, "f c h w -> 1 c f h w")
        masked_image_latents = rearrange(masked_image_latents, "f c h w -> 1 c f h w")

        mask = torch.cat([mask] * 2) if do_classifier_free_guidance else mask
        masked_image_latents = (
            torch.cat([masked_image_latents] * 2) if do_classifier_free_guidance else masked_image_latents
        )
        return mask, masked_image_latents

    def prepare_image_latents(self, images, device, dtype, generator, do_classifier_free_guidance):
        images = images.to(device=device, dtype=dtype)
        image_latents = self.vae.encode(images).latent_dist.sample(generator=generator)
        image_latents = (image_latents - self.vae.config.shift_factor) * self.vae.config.scaling_factor
        image_latents = rearrange(image_latents, "f c h w -> 1 c f h w")
        image_latents = torch.cat([image_latents] * 2) if do_classifier_free_guidance else image_latents

        return image_latents

    def set_progress_bar_config(self, **kwargs):
        if not hasattr(self, "_progress_bar_config"):
            self._progress_bar_config = {}
        self._progress_bar_config.update(kwargs)

    @staticmethod
    def paste_surrounding_pixels_back(decoded_latents, pixel_values, masks, device, weight_dtype):
        # Paste the surrounding pixels back, because we only want to change the mouth region
        pixel_values = pixel_values.to(device=device, dtype=weight_dtype)
        masks = masks.to(device=device, dtype=weight_dtype)
        combined_pixel_values = decoded_latents * masks + pixel_values * (1 - masks)
        return combined_pixel_values

    @staticmethod
    def pixel_values_to_images(pixel_values: torch.Tensor):
        pixel_values = rearrange(pixel_values, "f c h w -> f h w c")
        pixel_values = (pixel_values / 2 + 0.5).clamp(0, 1)
        images = (pixel_values * 255).to(torch.uint8)
        images = images.cpu().numpy()
        return images

    # ------------------- AANGEPASTE METHODE START -------------------
    def affine_transform_video(self, video_frames: np.ndarray):
        faces_list = [] # Renamed to avoid conflict with variable 'faces' later
        boxes_list = [] # Renamed
        affine_matrices_list = [] # Renamed
        
        # Gebruik de lengte van video_frames als die bestaat, anders 0.
        num_frames_to_process = len(video_frames) if video_frames is not None and len(video_frames) > 0 else 0
        print(f"Affine transforming {num_frames_to_process} faces...")

        # Variabelen om de laatst succesvolle detectie op te slaan
        last_successful_face = None
        last_successful_box = None
        last_successful_affine_matrix = None
        # Een vlag om te weten of we ooit een gezicht hebben gedetecteerd
        ever_detected_a_face = False

        # Alleen itereren als er frames zijn
        if num_frames_to_process > 0:
            for frame_idx, frame in enumerate(tqdm.tqdm(video_frames, desc="Affine transforming faces")):
                try:
                    face, box, affine_matrix = self.image_processor.affine_transform(frame)
                    # Sla de succesvolle detectie op
                    last_successful_face = face
                    last_successful_box = box
                    last_successful_affine_matrix = affine_matrix
                    ever_detected_a_face = True # Markeer dat we tenminste één keer een gezicht hebben gevonden
                    
                    faces_list.append(face)
                    boxes_list.append(box)
                    affine_matrices_list.append(affine_matrix)

                except RuntimeError as e:
                    if "Face not detected" in str(e):
                        if ever_detected_a_face and last_successful_face is not None:
                            # Hergebruik de laatst bekende succesvolle data
                            logger.warning(f"Face not detected in frame {frame_idx}. Re-using last known face data.")
                            faces_list.append(last_successful_face)
                            boxes_list.append(last_successful_box)
                            affine_matrices_list.append(last_successful_affine_matrix)
                        else:
                            # Geen gezicht gedetecteerd en geen vorige data beschikbaar (bv. eerste frames)
                            # Of we hebben nog nooit een gezicht gedetecteerd.
                            # Hier moeten we de error opnieuw gooien, anders hebben we geen startpunt.
                            logger.error(f"Face not detected in frame {frame_idx} and no previous face data available or no face detected yet. Stopping.")
                            raise e # Gooi de error opnieuw om het proces te stoppen
                    else:
                        # Her-raise andere RuntimeErrors die niet "Face not detected" zijn
                        raise e
            
            if not ever_detected_a_face and num_frames_to_process > 0 :
                # Als er frames waren, maar we hebben nooit een gezicht gedetecteerd.
                raise RuntimeError("Face not detected in any of the initial frames. Cannot proceed.")

        # Alleen stacken als er faces zijn (kan leeg zijn als video_frames leeg was of geen gezichten gevonden)
        if faces_list:
            faces_tensor = torch.stack(faces_list)
        elif num_frames_to_process > 0 and not ever_detected_a_face :
            # Dit geval zou al afgevangen moeten zijn door de RuntimeError hierboven,
            # maar voor de zekerheid, als faces_list leeg is maar er waren frames...
            raise RuntimeError("No faces were successfully processed or detected.")
        else:
            # Als video_frames initieel leeg was, of om een andere reden faces_list leeg is
            # maar we niet in een error state zijn beland.
            faces_tensor = torch.empty(0, dtype=torch.float32, device=self._execution_device) # Pas dtype en device aan indien nodig


        return faces_tensor, boxes_list, affine_matrices_list
    # ------------------- AANGEPASTE METHODE EINDE -------------------

    def restore_video(self, faces: torch.Tensor, video_frames: np.ndarray, boxes: list, affine_matrices: list):
        # Check if there are faces to restore
        if faces is None or faces.nelement() == 0:
            logger.warning("No faces to restore. Returning original video frames or empty array.")
            # Afhankelijk van de gewenste gedrag, retourneer originele frames of lege array
            # Als er geen video_frames zijn, kan video_frames[:len(faces)] een error geven.
            if video_frames is not None and len(video_frames) > 0:
                 return video_frames[:0] # retourneer een leeg np.array met de juiste shape (0 elementen)
            return np.array([])


        video_frames = video_frames[: len(faces)] # Dit kan een error geven als video_frames korter is dan len(faces)
                                               # of als video_frames None is.
                                               # Echter, loop_video zou dit moeten afhandelen.
        out_frames = []
        print(f"Restoring {len(faces)} faces...")
        for index, face in enumerate(tqdm.tqdm(faces)):
            # Controleer of boxes en affine_matrices data hebben voor deze index
            if index >= len(boxes) or index >= len(affine_matrices):
                logger.error(f"Mismatch in lengths of faces, boxes, or affine_matrices at index {index}. Skipping restore for this frame.")
                # Optie: voeg het originele video_frame toe, of sla over, of stop.
                # Voor nu, voegen we het onbewerkte frame toe als het bestaat.
                if index < len(video_frames):
                    out_frames.append(video_frames[index])
                continue

            x1, y1, x2, y2 = boxes[index]
            height = int(y2 - y1)
            width = int(x2 - x1)

            # Voeg een check toe voor ongeldige hoogte/breedte (kan gebeuren als box data corrupt is)
            if height <= 0 or width <= 0:
                logger.warning(f"Invalid dimensions for face resize at index {index}: height={height}, width={width}. Using original frame.")
                if index < len(video_frames):
                     out_frames.append(video_frames[index])
                continue

            face = torchvision.transforms.functional.resize(
                face, size=(height, width), interpolation=transforms.InterpolationMode.BICUBIC, antialias=True
            )
            # Controleer of video_frames[index] en affine_matrices[index] bestaan
            if index < len(video_frames) and self.image_processor.restorer is not None:
                 out_frame = self.image_processor.restorer.restore_img(video_frames[index], face, affine_matrices[index])
                 out_frames.append(out_frame)
            elif index < len(video_frames): # Als restorer None is, misschien het onbewerkte frame toevoegen?
                 logger.warning(f"Image restorer is None. Appending original frame at index {index}.")
                 out_frames.append(video_frames[index])
            else:
                 logger.error(f"Video frame not available for index {index} during restoration.")


        if not out_frames: # Als out_frames leeg is
            logger.warning("No frames were restored. Returning an empty array.")
            return np.array([])
        return np.stack(out_frames, axis=0)

    def loop_video(self, whisper_chunks: list, video_frames: np.ndarray):
        # Controleer of video_frames een NumPy array is en niet None
        if video_frames is None or not isinstance(video_frames, np.ndarray) or video_frames.size == 0:
            logger.error("video_frames is None, not a NumPy array, or empty in loop_video. Cannot proceed.")
            # Retourneer lege structuren of gooi een error, afhankelijk van hoe de __call__ methode dit afhandelt.
            # Voor nu, retourneren we lege structuren zodat de __call__ methode kan beslissen.
            return np.array([]), torch.empty(0), [], []


        # If the audio is longer than the video, we need to loop the video
        if len(whisper_chunks) > len(video_frames):
            # Eerst affine transformatie op de originele video_frames
            # De aangepaste affine_transform_video wordt hier aangeroepen
            faces, boxes, affine_matrices = self.affine_transform_video(video_frames)

            # Controleer of de initiële transformatie succesvol was
            if faces.nelement() == 0 and len(video_frames) > 0:
                logger.error("Initial affine transformation failed to produce faces in loop_video. Cannot loop.")
                # Het is waarschijnlijk het beste om hier te stoppen of een duidelijke fout te retourneren
                # omdat loopen zonder basisdata zinloos is.
                # Voor nu, retourneren we de (mislukte) resultaten van de eerste poging.
                return video_frames, faces, boxes, affine_matrices


            num_loops = math.ceil(len(whisper_chunks) / len(video_frames))
            loop_video_frames_list = [] # Renamed
            loop_faces_list = [] # Renamed
            loop_boxes_list = boxes[:] # Kopieer de originele lijst
            loop_affine_matrices_list = affine_matrices[:] # Kopieer

            # Voeg de originele (niet-geloopte) data toe als startpunt
            loop_video_frames_list.append(video_frames)
            loop_faces_list.append(faces)
            # boxes en affine_matrices zijn al geïnitialiseerd met de eerste set

            # Begin de loop vanaf de tweede iteratie als num_loops > 1
            for i in range(1, num_loops): # Start vanaf 1 omdat de 0-de iteratie al is gedaan
                current_video_frames = video_frames if i % 2 == 1 else video_frames[::-1].copy() # .copy() voor omgekeerd
                current_faces = faces if i % 2 == 1 else faces.flip(0)
                current_boxes = boxes if i % 2 == 1 else boxes[::-1]
                current_affine_matrices = affine_matrices if i % 2 == 1 else affine_matrices[::-1]
                
                loop_video_frames_list.append(current_video_frames)
                loop_faces_list.append(current_faces)
                loop_boxes_list.extend(current_boxes) # Gebruik extend voor lijsten
                loop_affine_matrices_list.extend(current_affine_matrices) # Gebruik extend

            # Gebruik de geneste lijsten correct
            final_video_frames = np.concatenate(loop_video_frames_list, axis=0)[: len(whisper_chunks)]
            final_faces = torch.cat(loop_faces_list, dim=0)[: len(whisper_chunks)]
            final_boxes = loop_boxes_list[: len(whisper_chunks)]
            final_affine_matrices = loop_affine_matrices_list[: len(whisper_chunks)]

        else:
            # Audio is niet langer, of even lang/korter dan video
            final_video_frames = video_frames[: len(whisper_chunks)]
            # De aangepaste affine_transform_video wordt hier aangeroepen
            final_faces, final_boxes, final_affine_matrices = self.affine_transform_video(final_video_frames)

        return final_video_frames, final_faces, final_boxes, final_affine_matrices


    @torch.no_grad()
    def __call__(
        self,
        video_path: str,
        audio_path: str,
        video_out_path: str,
        video_mask_path: str = None,
        num_frames: int = 16,
        video_fps: int = 25,
        audio_sample_rate: int = 16000,
        height: Optional[int] = None,
        width: Optional[int] = None,
        num_inference_steps: int = 20,
        guidance_scale: float = 1.5,
        weight_dtype: Optional[torch.dtype] = torch.float16,
        eta: float = 0.0,
        mask_image_path: str = "latentsync/utils/mask.png",
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,
        callback_steps: Optional[int] = 1,
        **kwargs,
    ):
        is_train = self.denoising_unet.training
        self.denoising_unet.eval()

        check_ffmpeg_installed()

        # 0. Define call parameters
        batch_size = 1
        device = self._execution_device
        
        # Default height and width to unet config if not provided
        height = height or self.denoising_unet.config.sample_size * self.vae_scale_factor
        width = width or self.denoising_unet.config.sample_size * self.vae_scale_factor
        
        mask_image = load_fixed_mask(height, mask_image_path) # height moet hier al gedefinieerd zijn
        self.image_processor = ImageProcessor(height, device="cuda", mask_image=mask_image) # 'cuda' hardcoded, overweeg device
        self.set_progress_bar_config(desc=f"Sample frames: {num_frames}")

        # 1. Default height and width to unet (al hierboven gedaan)

        # 2. Check inputs
        self.check_inputs(height, width, callback_steps)

        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
        # corresponds to doing no classifier free guidance.
        do_classifier_free_guidance = guidance_scale > 1.0

        # 3. set timesteps
        self.scheduler.set_timesteps(num_inference_steps, device=device)
        timesteps = self.scheduler.timesteps

        # 4. Prepare extra step kwargs.
        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)

        whisper_feature = self.audio_encoder.audio2feat(audio_path)
        whisper_chunks = self.audio_encoder.feature2chunks(feature_array=whisper_feature, fps=video_fps)

        audio_samples = read_audio(audio_path)
        video_frames_np = read_video(video_path, use_decord=False) # Renamed to avoid conflict

        # Voeg een check toe of video_frames_np succesvol is geladen
        if video_frames_np is None or video_frames_np.size == 0:
            logger.error(f"Failed to read video from {video_path} or video is empty.")
            # Hier zou je een error kunnen gooien of een lege output retourneren
            # Voor Gradio is een gr.Error() geschikter.
            raise ValueError(f"Failed to read video from {video_path} or video is empty.")


        # De aangepaste loop_video wordt hier aangeroepen
        processed_video_frames, faces_tensor, boxes_list, affine_matrices_list = self.loop_video(whisper_chunks, video_frames_np)

        # Na loop_video, controleer of we geldige data hebben ontvangen.
        # Specifiek, als faces_tensor leeg is, betekent dit waarschijnlijk dat affine_transform_video (binnen loop_video)
        # geen gezichten kon verwerken, zelfs niet met de fallback.
        if faces_tensor is None or faces_tensor.nelement() == 0:
            logger.error("No faces were processed or detected after video looping and affine transformation. Cannot proceed with inference.")
            # Dit is een kritieke fout; de rest van de pipeline kan niet doorgaan.
            # Afhankelijk van de context (bv. Gradio app), wil je hier misschien een specifieke error gooien.
            raise RuntimeError("Critical error: No face data available for inference after initial processing.")


        synced_video_frames_list = [] # Renamed

        num_channels_latents = self.vae.config.latent_channels

        # Prepare latent variables
        all_latents = self.prepare_latents(
            batch_size,
            len(whisper_chunks), # Gebruik lengte van whisper_chunks, aangezien video hierop is afgestemd
            num_channels_latents,
            height,
            width,
            weight_dtype,
            device,
            generator,
        )

        num_inferences = math.ceil(len(whisper_chunks) / num_frames)
        for i in tqdm.tqdm(range(num_inferences), desc="Doing inference..."):
            start_idx = i * num_frames
            end_idx = min((i + 1) * num_frames, len(whisper_chunks)) # Voorkom index out of bounds

            if self.denoising_unet.add_audio_layer:
                # Zorg ervoor dat we niet buiten de grenzen van whisper_chunks gaan
                current_whisper_chunks = whisper_chunks[start_idx:end_idx]
                if not current_whisper_chunks: # Als de lijst leeg is, sla over
                    logger.warning(f"Skipping inference for batch {i} due to empty whisper_chunks.")
                    continue

                audio_embeds = torch.stack(current_whisper_chunks)
                audio_embeds = audio_embeds.to(device, dtype=weight_dtype)
                if do_classifier_free_guidance:
                    null_audio_embeds = torch.zeros_like(audio_embeds)
                    audio_embeds = torch.cat([null_audio_embeds, audio_embeds])
            else:
                audio_embeds = None
            
            # Zorg ervoor dat we niet buiten de grenzen van faces_tensor gaan
            if start_idx >= faces_tensor.shape[0]:
                logger.warning(f"Skipping inference for batch {i} as start_idx is out of bounds for faces_tensor.")
                continue
            inference_faces = faces_tensor[start_idx:end_idx]
            
            # Zorg ervoor dat inference_faces niet leeg is
            if inference_faces.nelement() == 0:
                logger.warning(f"Skipping inference for batch {i} due to empty inference_faces.")
                continue


            latents = all_latents[:, :, start_idx:end_idx]
            
            # Controleer of latents een geldige shape heeft, vooral de frame dimensie
            if latents.shape[2] == 0: # Index 2 is de frame dimensie (b c f h w)
                logger.warning(f"Skipping inference for batch {i} due to zero frames in latents.")
                continue


            ref_pixel_values, masked_pixel_values, masks = self.image_processor.prepare_masks_and_masked_images(
                inference_faces, affine_transform=False
            )

            # 7. Prepare mask latent variables
            mask_latents, masked_image_latents = self.prepare_mask_latents(
                masks,
                masked_pixel_values,
                height,
                width,
                weight_dtype,
                device,
                generator,
                do_classifier_free_guidance,
            )

            # 8. Prepare image latents
            ref_latents = self.prepare_image_latents(
                ref_pixel_values,
                device,
                weight_dtype,
                generator,
                do_classifier_free_guidance,
            )

            # 9. Denoising loop
            num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order
            with self.progress_bar(total=num_inference_steps) as progress_bar:
                for j, t in enumerate(timesteps):
                    # expand the latents if we are doing classifier free guidance
                    denoising_unet_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents

                    denoising_unet_input = self.scheduler.scale_model_input(denoising_unet_input, t)

                    # concat latents, mask, masked_image_latents in the channel dimension
                    denoising_unet_input = torch.cat(
                        [denoising_unet_input, mask_latents, masked_image_latents, ref_latents], dim=1
                    )

                    # predict the noise residual
                    noise_pred = self.denoising_unet(
                        denoising_unet_input, t, encoder_hidden_states=audio_embeds
                    ).sample

                    # perform guidance
                    if do_classifier_free_guidance:
                        noise_pred_uncond, noise_pred_audio = noise_pred.chunk(2)
                        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_audio - noise_pred_uncond)

                    # compute the previous noisy sample x_t -> x_t-1
                    latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample

                    # call the callback, if provided
                    if j == len(timesteps) - 1 or ((j + 1) > num_warmup_steps and (j + 1) % self.scheduler.order == 0):
                        progress_bar.update()
                        if callback is not None and j % callback_steps == 0:
                            callback(j, t, latents)

            # Recover the pixel values
            decoded_latents = self.decode_latents(latents) # latents is nu (b, c, f_batch, h, w)
            
            # ref_pixel_values en masks moeten overeenkomen met de huidige batch frames
            # In prepare_masks_and_masked_images worden ze per batch (inference_faces) gemaakt.
            # Zorg ervoor dat de (1 - masks) ook correct gebroadcast wordt of dezelfde frame count heeft.
            # masks komt van self.image_processor.prepare_masks_and_masked_images(inference_faces)
            # dus zou de juiste frame count moeten hebben voor deze batch.
            # `1 - masks` is correct.
            decoded_latents = self.paste_surrounding_pixels_back(
                decoded_latents, ref_pixel_values, 1 - masks, device, weight_dtype
            )
            synced_video_frames_list.append(decoded_latents)

        # Na de inference loop, controleer of synced_video_frames_list items bevat
        if not synced_video_frames_list:
            logger.error("No frames were generated during inference. Cannot restore video.")
            # Afhankelijk van of processed_video_frames (de input voor restore) content heeft,
            # kun je besluiten wat te doen. Als processed_video_frames leeg is, is er sowieso een probleem.
            if processed_video_frames is None or processed_video_frames.size == 0:
                 raise RuntimeError("Critical error: No input frames available and no frames generated for video restoration.")
            else:
                 # Misschien wil je de originele (niet-gesynchroniseerde) frames teruggeven als er geen output was?
                 # Of een error gooien. Voor nu een error.
                 raise RuntimeError("No frames were generated by the lipsync model. Cannot create output video.")


        # De restore_video functie heeft nu de tensor van alle gesynchroniseerde frames nodig
        final_synced_frames_tensor = torch.cat(synced_video_frames_list, dim=0) # dim=0 omdat decoded_latents (f,c,h,w) is

        # Roep restore_video aan met de geconcateneerde tensor
        final_output_video_np = self.restore_video(final_synced_frames_tensor, processed_video_frames, boxes_list, affine_matrices_list)

        # Controleer of final_output_video_np content heeft
        if final_output_video_np is None or final_output_video_np.size == 0:
            logger.error("Video restoration resulted in empty video. Cannot write output.")
            raise RuntimeError("Failed to restore video frames. Output is empty.")


        audio_samples_remain_length = int(final_output_video_np.shape[0] / video_fps * audio_sample_rate)
        
        # Controleer of audio_samples een tensor is voordat je .cpu().numpy() aanroept
        if isinstance(audio_samples, torch.Tensor):
            audio_samples_np = audio_samples[:audio_samples_remain_length].cpu().numpy()
        elif isinstance(audio_samples, np.ndarray):
            audio_samples_np = audio_samples[:audio_samples_remain_length]
        else:
            logger.error(f"audio_samples is of unexpected type: {type(audio_samples)}")
            audio_samples_np = np.array([]) # Lege array om verdere errors te voorkomen


        if is_train:
            self.denoising_unet.train()

        temp_dir = "temp_latentsync_output" # Uniekere naam om conflicten te vermijden
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        os.makedirs(temp_dir, exist_ok=True)

        # Gebruik final_output_video_np en audio_samples_np
        write_video(os.path.join(temp_dir, "video.mp4"), final_output_video_np, fps=video_fps) # fps was 25, gebruik video_fps

        sf.write(os.path.join(temp_dir, "audio.wav"), audio_samples_np, audio_sample_rate)

        command = f"ffmpeg -y -loglevel error -nostdin -i {os.path.join(temp_dir, 'video.mp4')} -i {os.path.join(temp_dir, 'audio.wav')} -c:v libx264 -preset medium -crf 18 -c:a aac -b:a 192k {video_out_path}"
        try:
            subprocess.run(command, shell=True, check=True) # check=True gooit error bij mislukken
            logger.info(f"Processing completed. Output video at: {video_out_path}")
        except subprocess.CalledProcessError as e:
            logger.error(f"FFmpeg command failed: {e}")
            logger.error(f"FFmpeg stdout: {e.stdout}")
            logger.error(f"FFmpeg stderr: {e.stderr}")
            # Overweeg de temp bestanden niet te verwijderen voor debuggen
            raise RuntimeError(f"FFmpeg command failed to merge video and audio. Check logs. Temp files in {temp_dir}")
        finally:
            # Optioneel: verwijder temp_dir alleen bij succes, of altijd.
            # Voor nu, laten we het altijd proberen te verwijderen.
            if os.path.exists(temp_dir):
                 try:
                     shutil.rmtree(temp_dir)
                 except Exception as e_rm:
                     logger.warning(f"Could not remove temporary directory {temp_dir}: {e_rm}")
        
        return video_out_path # Retourneer het pad naar de output video